{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ae9558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "my_df = pd.read_csv(\"Reviews.csv\")\n",
    "\n",
    "# Displaying the first 5 rows of the DataFrame\n",
    "print(my_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bdfd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb464e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Train  Validation   Test\n",
      "Label                              \n",
      "Positive  266240       88512  89025\n",
      "Negative   49278       16578  16181\n",
      "Neutral    25554        8601   8485\n",
      "Training Set Label Distribution: Positive    0.780598\n",
      "Negative    0.144480\n",
      "Neutral     0.074923\n",
      "Name: reviews, dtype: float64\n",
      "Validation Set Label Distribution: Positive    0.778531\n",
      "Negative    0.145816\n",
      "Neutral     0.075652\n",
      "Name: reviews, dtype: float64\n",
      "Test Set Label Distribution: Positive    0.783044\n",
      "Negative    0.142324\n",
      "Neutral     0.074632\n",
      "Name: reviews, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# mapping the ratings to labels\n",
    "my_df['reviews'] = my_df['Score'].apply(lambda x: 'Negative' if x in [1, 2] else 'Neutral' if x == 3 else 'Positive')\n",
    "\n",
    "# splitting the dataset into training, validation, and test sets\n",
    "train_val_my_df, test_my_df = train_test_split(my_df, test_size=0.2, random_state=42)\n",
    "train_my_df, val_my_df = train_test_split(train_val_my_df, test_size=0.25, random_state=42)\n",
    "\n",
    "# calculate label counts for each split\n",
    "train_label_counts = train_my_df['reviews'].value_counts()\n",
    "val_label_counts = val_my_df['reviews'].value_counts()\n",
    "test_label_counts = test_my_df['reviews'].value_counts()\n",
    "\n",
    "# creating table of label counts for each split\n",
    "counts_table = pd.concat([train_label_counts, val_label_counts, test_label_counts], axis=1, sort=False)\n",
    "counts_table.columns = ['Train', 'Validation', 'Test']\n",
    "counts_table.index.name = 'Label'\n",
    "print(counts_table) \n",
    "\n",
    "# Checking label distribution across splits\n",
    "print('Training Set Label Distribution:', train_label_counts/len(train_my_df))\n",
    "print('Validation Set Label Distribution:', val_label_counts/len(val_my_df))\n",
    "print('Test Set Label Distribution:', test_label_counts/len(test_my_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d9bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "Number of documents: 0\n",
      "Top terms: ['these', 'they', 'are', 'the', 'them', 'and', 'to', 'of', 'for', 'in']\n",
      "Cluster 1:\n",
      "Number of documents: 2\n",
      "Top terms: ['food', 'she', 'the', 'dog', 'he', 'and', 'to', 'it', 'her', 'my']\n",
      "Cluster 2:\n",
      "Number of documents: 0\n",
      "Top terms: ['coffee', 'the', 'it', 'this', 'is', 'and', 'cup', 'of', 'to', 'for']\n",
      "Cluster 3:\n",
      "Number of documents: 1\n",
      "Top terms: ['br', 'the', 'it', 'and', 'to', 'of', 'is', 'you', 'this', 'in']\n",
      "Cluster 4:\n",
      "Number of documents: 8\n",
      "Top terms: ['the', 'it', 'and', 'this', 'is', 'to', 'of', 'for', 'in', 'tea']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 0: Vectorise text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(my_df['Text'])\n",
    "\n",
    "# Step 1: Pick k random \"centroids\"\n",
    "k = 5\n",
    "my_kmeans = KMeans(n_clusters=k, init='random', max_iter=100, n_init=1, random_state=42)\n",
    "\n",
    "# Step 2 and 3: Assign each vector to its closest centroid and recalculate the centroids based on the closest vectors\n",
    "my_kmeans.fit(X)\n",
    "\n",
    "\n",
    "\n",
    "def get_top_terms(cluster_centroid, feature_names_out, n=10):\n",
    "    # Get the top n terms for a given centroid\n",
    "    sort_terms = cluster_centroid.argsort()[::-1]\n",
    "    return [feature_names_out[i] for i in sort_terms[:n]]\n",
    "\n",
    "\n",
    "\n",
    "# Print the cluster assignments and the top terms in each cluster\n",
    "cluster_labels = my_kmeans.labels_\n",
    "cluster_centroids = my_kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    cluster_documents = [doc for j, doc in enumerate(my_df) if cluster_labels[j] == i]\n",
    "    print(\"Number of documents:\", len(cluster_documents))\n",
    "    top_term_cluster = get_top_terms(cluster_centroids[i], vectorizer.get_feature_names_out(), n=10)\n",
    "    print(\"Top terms:\", top_term_cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70f7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters 0:\n",
      "Number of docs: 0\n",
      "Sample docs:\n",
      "Top terms: ['these', 'they', 'are', 'the', 'them']\n",
      "Clusters 1:\n",
      "Number of docs: 2\n",
      "Sample docs:\n",
      " - Id\n",
      " - Text\n",
      "Top terms: ['food', 'she', 'the', 'dog', 'he']\n",
      "Clusters 2:\n",
      "Number of docs: 0\n",
      "Sample docs:\n",
      "Top terms: ['coffee', 'the', 'it', 'this', 'is']\n",
      "Clusters 3:\n",
      "Number of docs: 1\n",
      "Sample docs:\n",
      " - reviews\n",
      "Top terms: ['br', 'the', 'it', 'and', 'to']\n",
      "Clusters 4:\n",
      "Number of docs: 8\n",
      "Sample docs:\n",
      " - ProductId\n",
      " - UserId\n",
      " - ProfileName\n",
      " - HelpfulnessNumerator\n",
      " - HelpfulnessDenominator\n",
      "Top terms: ['the', 'it', 'and', 'this', 'is']\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print(\"Clusters %d:\" % i)\n",
    "    cluster_documents = [doc for j, doc in enumerate(my_df) if cluster_labels[j] == i]\n",
    "    print(\"Number of docs:\", len(cluster_documents))\n",
    "    print(\"Sample docs:\")\n",
    "    for doc in cluster_documents[:5]:\n",
    "        print(\" -\", doc)\n",
    "    top_term_cluster = get_top_terms(cluster_centroids[i], vectorizer.get_feature_names_out(), n=5)\n",
    "    print(\"Top terms:\", top_term_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f4517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True_Label  Negative  Neutral  Positive\n",
      "Cluster                                \n",
      "0              13091     7093     91863\n",
      "1               6642     3125     43623\n",
      "2               6304     4459     38576\n",
      "3              11798     7588     53238\n",
      "4              44202    20375    216477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a new DataFrame with cluster assignments and corresponding true labels\n",
    "my_cluster_df = pd.DataFrame({'Cluster': cluster_labels, 'True_Label': my_df['reviews']})\n",
    "\n",
    "# Constructing the confusion matrix using pd.crosstab()\n",
    "my_confusion_matrix = pd.crosstab(my_cluster_df['Cluster'], my_cluster_df['True_Label'])\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print(my_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d889783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1: Dummy Classifier with strategy=\"most_frequent\"\n",
      "Accuracy: 0.7830435126791039\n",
      "Precision: 0.26101450422636796\n",
      "Recall: 0.3333333333333333\n",
      "F1-score: 0.2927741273505791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline 1: Dummy Classifier with strategy=\"most_frequent\"\n",
    "dummy_most_freq = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_most_freq.fit(X_train_set, y_train_set)\n",
    "y_pred_dummy_most_freq = dummy_most_freq.predict(X_test_set)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "print(\"Baseline 1: Dummy Classifier with strategy=\\\"most_frequent\\\"\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_dummy_most_freq))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_dummy_most_freq, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_dummy_most_freq, average='macro', zero_division=0))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_dummy_most_freq, average='macro', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1677abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 2: Dummy Classifier with strategy=\"stratified\"\n",
      "Accuracy: 0.6354768627244021\n",
      "Precision: 0.3330945732715712\n",
      "Recall: 0.3331060757768482\n",
      "F1-score: 0.3330901035831019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline 2: Dummy Classifier with strategy=\"stratified\"\n",
    "dummy_strat = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_strat.fit(X_train_set, y_train_set)\n",
    "y_pred_dummy_strat = dummy_strat.predict(X_test_set)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "print(\"Baseline 2: Dummy Classifier with strategy=\\\"stratified\\\"\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_dummy_strat))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_dummy_strat, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_dummy_strat, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_dummy_strat, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b2cddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 3: Logistic Regression with One-hot vectorization\n",
      "Accuracy: 0.8874932932246176\n",
      "Precision: 0.7609740991228365\n",
      "Recall: 0.6880308626013472\n",
      "F1-score: 0.7156055112479064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert the 'Text' column to strings\n",
    "my_df['Text'] = my_df['Text'].astype(str)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the text data into a one-hot encoded matrix\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train_onehot = vectorizer.fit_transform(X_train_set)\n",
    "X_test_onehot = vectorizer.transform(X_test_set)\n",
    "\n",
    "# Baseline 3: Logistic Regression with One-hot vectorization\n",
    "logreg_onehot = LogisticRegression(max_iter=2000)\n",
    "logreg_onehot.fit(X_train_onehot, y_train_set)\n",
    "y_pred_logreg_onehot = logreg_onehot.predict(X_test_onehot)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "print(\"Baseline 3: Logistic Regression with One-hot vectorization\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_logreg_onehot))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_logreg_onehot, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_logreg_onehot, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_logreg_onehot, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cac241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 4: Logistic Regression with TF-IDF vectorization\n",
      "Accuracy: 0.8820399152087677\n",
      "Precision: 0.7572256001312428\n",
      "Recall: 0.6496027982372178\n",
      "F1-score: 0.6816118772721121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert the 'Text' column to strings\n",
    "my_df['Text'] = my_df['Text'].astype(str)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the text data into a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_set)\n",
    "X_test_tfidf = vectorizer.transform(X_test_set)\n",
    "\n",
    "# Baseline 4: Logistic Regression with TF-IDF vectorization\n",
    "logreg_tfidf = LogisticRegression(max_iter=1000)\n",
    "logreg_tfidf.fit(X_train_tfidf, y_train_set)\n",
    "y_pred_logreg_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "print(\"Baseline 4: Logistic Regression with TF-IDF vectorization\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_logreg_tfidf))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_logreg_tfidf, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_logreg_tfidf, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_logreg_tfidf, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert the 'Text' column to strings\n",
    "my_df['Text'] = my_df['Text'].astype(str)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the text data into a one-hot encoded matrix\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train_onehot = vectorizer.fit_transform(X_train_set)\n",
    "X_test_onehot = vectorizer.transform(X_test_set)\n",
    "\n",
    "# Baseline 5: SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings)\n",
    "SVC_onehot = SVC()\n",
    "SVC_onehot.fit(X_train_onehot, y_train_set)\n",
    "y_pred_SVC_onehot = SVC_onehot.predict(X_test_onehot)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "print(\"Baseline 5: SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_SVC_onehot))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_SVC_onehot, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_SVC_onehot, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_SVC_onehot, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7090084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_logreg_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/xsy1gzx10wb944j22_7y27km0000gn/T/ipykernel_13805/2693534341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'Dummy (Most Frequent)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_dummy_most_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'Dummy (Stratified)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_dummy_strat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'LogReg (One-hot)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_logreg_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'LogReg (TF-IDF)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_logreg_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'SVC (One-hot)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_SVC_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_logreg_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Store the evaluation metrics for each classifier\n",
    "results = [\n",
    "    ('Dummy (Most Frequent)', y_test_set, y_pred_dummy_most_freq),\n",
    "    ('Dummy (Stratified)', y_test_set, y_pred_dummy_strat),\n",
    "    ('LogReg (One-hot)', y_test_set, y_pred_logreg_onehot),\n",
    "    ('LogReg (TF-IDF)', y_test_set, y_pred_logreg_tfidf),\n",
    "    ('SVC (One-hot)', y_test_set, y_pred_SVC_onehot),\n",
    "]\n",
    "\n",
    "# Calculate and store the accuracy, precision, recall, and F1-score for each classifier\n",
    "metrics = []\n",
    "for classifier_name, y_true, y_pred in results:\n",
    "    metrics.append(\n",
    "        (\n",
    "            classifier_name,\n",
    "            accuracy_score(y_true, y_pred),\n",
    "            precision_score(y_true, y_pred, average='macro'),\n",
    "            recall_score(y_true, y_pred, average='macro'),\n",
    "            f1_score(y_true, y_pred, average='macro'),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create a DataFrame with the evaluation metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics,\n",
    "    columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    ")\n",
    "\n",
    "# Display the evaluation metrics table\n",
    "print(metrics_df)\n",
    "\n",
    "# Find the best classifier based on macro F1-score\n",
    "best_classifier = max(results, key=lambda x: f1_score(x[1], x[2], average='macro'))\n",
    "\n",
    "# Plot the F1-score for each class of the best classifier\n",
    "f1_scores = f1_score(best_classifier[1], best_classifier[2], average=None)\n",
    "class_labels = sorted(list(set(y_test_set)))\n",
    "plt.bar(class_labels, f1_scores)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title(f'F1-score for each class ({best_classifier[0]})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74032784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Classifier with One-hot vectorization\n",
      "Accuracy: 0.847059133968388\n",
      "Precision: 0.6609357405634726\n",
      "Recall: 0.6363340728519842\n",
      "F1-score: 0.646771994900286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(my_df['Text'], my_df['reviews'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the text data into a one-hot encoded matrix\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train_onehot = vectorizer.fit_transform(X_train_set)\n",
    "X_test_onehot = vectorizer.transform(X_test_set)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_onehot, y_train_set)\n",
    "\n",
    "# Evaluate the classifier using accuracy, precision, recall, and F1-score\n",
    "y_pred_mnb = mnb.predict(X_test_onehot)\n",
    "print(\"Multinomial Naive Bayes Classifier with One-hot vectorization\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_set, y_pred_mnb))\n",
    "print(\"Precision:\", precision_score(y_test_set, y_pred_mnb, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_set, y_pred_mnb, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test_set, y_pred_mnb, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0293d22",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/xsy1gzx10wb944j22_7y27km0000gn/T/ipykernel_13805/412839902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_features_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmin_df_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t8/xsy1gzx10wb944j22_7y27km0000gn/T/ipykernel_13805/412839902.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(c, sublinear_tf, max_features, min_df)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to train and evaluate a model with the given parameters\n",
    "def train_and_evaluate(c, sublinear_tf, max_features, min_df):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=sublinear_tf, max_features=max_features, min_df=min_df)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_set)\n",
    "    X_test_tfidf = vectorizer.transform(X_test_set)\n",
    "    \n",
    "    logreg_tfidf = LogisticRegression(C=c, max_iter=1000)\n",
    "    logreg_tfidf.fit(X_train_tfidf, y_train_set)\n",
    "    y_pred_logreg_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_set, y_pred_logreg_tfidf)\n",
    "    precision = precision_score(y_test_set, y_pred_logreg_tfidf, average='macro',zero_division=0)\n",
    "    recall = recall_score(y_test_set, y_pred_logreg_tfidf, average='macro')\n",
    "    f1 = f1_score(y_test_set, y_pred_logreg_tfidf, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Parameter values to try\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "sublinear_tf_values = [True, False]\n",
    "max_features_values = [None, 5000, 10000, 20000, 50000]\n",
    "min_df_values = [1, 5, 10, 20]\n",
    "\n",
    "# Initialize variables to store the best parameters and scores\n",
    "best_param = None\n",
    "best_f1 = 0\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for c in C_values:\n",
    "    for sublinear_tf in sublinear_tf_values:\n",
    "        for max_features in max_features_values:\n",
    "            for min_df in min_df_values:\n",
    "                accuracy, precision, recall, f1 = train_and_evaluate(c, sublinear_tf, max_features, min_df)\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_param = (c, sublinear_tf, max_features, min_df)\n",
    "                    best_f1 = f1\n",
    "\n",
    "# Print the best parameters and scores\n",
    "print(\"Best parameters:\", best_param)\n",
    "print(\"Best F1-score:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f3dfe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/xsy1gzx10wb944j22_7y27km0000gn/T/ipykernel_14208/402696309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the vectorizer and fit it to the combined training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define the logistic regression model and fit it to the combined training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_set' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the vectorizer and fit it to the combined training and validation sets\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_set)\n",
    "\n",
    "# Define the logistic regression model and fit it to the combined training and validation sets\n",
    "logreg_tfidf = LogisticRegression()\n",
    "logreg_tfidf.fit(X_train_tfidf, y_train_set)\n",
    "\n",
    "# Transform the test set using the vectorizer and evaluate the model\n",
    "X_test_tfidf = vectorizer.transform(X_test_set)\n",
    "y_pred_logreg_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test_set, y_pred_logreg_tfidf)\n",
    "precision = precision_score(y_test_set, y_pred_logreg_tfidf, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test_set, y_pred_logreg_tfidf, average='macro')\n",
    "f1 = f1_score(y_test_set, y_pred_logreg_tfidf, average='macro')\n",
    "conf_mat = confusion_matrix(y_test_set, y_pred_logreg_tfidf)\n",
    "\n",
    "# Print the evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Macro-averaged precision:\", precision)\n",
    "print(\"Macro-averaged recall:\", recall)\n",
    "print(\"Macro-averaged F1-score:\", f1)\n",
    "print(\"Confusion matrix:\\n\", conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cd82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
